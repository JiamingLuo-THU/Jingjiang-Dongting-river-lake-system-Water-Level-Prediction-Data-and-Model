{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecef0237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from numpy import newaxis\n",
    "import datetime as dt\n",
    "from keras.layers import Dense, Activation, Dropout, LSTM\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import  load_model\n",
    "from  sklearn import  metrics\n",
    "import itertools \n",
    "\n",
    "class Timer():   \n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\tself.start_dt = None\n",
    "\n",
    "\tdef start(self):       \n",
    "\t\tself.start_dt = dt.datetime.now()\n",
    "\n",
    "\tdef stop(self):\n",
    "\t\tend_dt = dt.datetime.now()\n",
    "\t\tprint('Time taken: %s' % (end_dt - self.start_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df24403f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "\t\"\"\"LSTM 模型\"\"\"\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\tself.model = Sequential()\n",
    "\n",
    "\tdef load_model(self, filepath):\n",
    "\t\tprint('[Model] Loading model from file %s' % filepath)\n",
    "\t\tself.model = load_model(filepath)\n",
    "\n",
    "\tdef build_model(self, configs):\n",
    "\t\ttimer = Timer()\n",
    "\t\ttimer.start()\n",
    "\n",
    "\t\tfor layer in configs['model']['layers']:\n",
    "\t\t\tneurons = layer['neurons'] if 'neurons' in layer else None\n",
    "\t\t\tdropout_rate = layer['rate'] if 'rate' in layer else None\n",
    "\t\t\tactivation = layer['activation'] if 'activation' in layer else None\n",
    "\t\t\treturn_seq = layer['return_seq'] if 'return_seq' in layer else None\n",
    "\t\t\tinput_timesteps = layer['input_timesteps'] if 'input_timesteps' in layer else None\n",
    "\t\t\tinput_dim = layer['input_dim'] if 'input_dim' in layer else None\n",
    "\n",
    "\t\t\tif layer['type'] == 'dense':\n",
    "\t\t\t\tself.model.add(Dense(neurons, activation=activation))\n",
    "\t\t\tif layer['type'] == 'lstm':\n",
    "\t\t\t\tself.model.add(LSTM(neurons, input_shape=(input_timesteps, input_dim), return_sequences=return_seq))\n",
    "\t\t\tif layer['type'] == 'dropout':\n",
    "\t\t\t\tself.model.add(Dropout(dropout_rate))\n",
    "\n",
    "\t\tself.model.compile(loss=configs['model']['loss'], optimizer=configs['model']['optimizer'])\n",
    "\n",
    "\t\tprint('[Model] Model Compiled')\n",
    "\t\ttimer.stop()\n",
    "\t\t\n",
    "\t\treturn self.model\n",
    "\t\t\n",
    "\n",
    "\tdef train(self, x, y,x_val,y_val, epochs, batch_size, save_dir):\n",
    "\t\ttimer = Timer()\n",
    "\t\ttimer.start()\n",
    "\t\tprint('[Model] %s epochs, %s batch size' % (epochs, batch_size))\n",
    "\t\t\n",
    "\t\tsave_fname = os.path.join(save_dir, '%s-e%s.h5' % (dt.datetime.now().strftime('%d%m%Y-%H%M%S'), str(epochs)))\n",
    "\t\tcallbacks = [\n",
    "\t\t\tEarlyStopping(monitor='val_loss', min_delta=1e-5, patience=4),\n",
    "\t\t\tModelCheckpoint(filepath=save_fname, monitor='val_loss', save_best_only=True)\n",
    "\t\t]\n",
    "\n",
    "\t\tself.model.fit(\n",
    "\t\t\tx,\n",
    "\t\t\ty,\n",
    "\t\t\tvalidation_data =(x_val,y_val),\n",
    "\t\t\tepochs=epochs,\n",
    "\t\t\tbatch_size=batch_size,\n",
    "\t\t\tcallbacks=callbacks\n",
    "\t\t)\n",
    "\n",
    "\t\ttimer.stop()\n",
    "\n",
    "\tdef predict_point_by_point(self, data,debug=False):\n",
    "\t\tif debug == False:\n",
    "\t\t\tprint('[Model] Predicting Point-by-Point...')\n",
    "\t\t\tpredicted = self.model.predict(data)\n",
    "\t\t\tpredicted = np.reshape(predicted, (predicted.size,))\n",
    "\t\telse:\n",
    "\t\t\tprint('[Model] Predicting Point-by-Point...')\n",
    "\t\t\tprint (np.array(data).shape)\n",
    "\t\t\tpredicted = self.model.predict(data)\n",
    "\t\t\tprint (np.array(predicted).shape)\n",
    "\t\t\tpredicted = np.reshape(predicted, (predicted.size,))\n",
    "\t\t\tprint (np.array(predicted).shape)\n",
    "\t\treturn predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3156192",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "\n",
    "    def __init__(self, filename, split, cols):\n",
    "        dataframe = pd.read_csv(filename)\n",
    "        i_split = int(len(dataframe) * split)\n",
    "\n",
    "        self.data_train = dataframe.get(cols).values[:i_split]\n",
    "        self.data_test  = dataframe.get(cols).values[i_split:]\n",
    "        self.all_data = dataframe.get(cols)\n",
    "        self.len_all  = len(self.all_data)\n",
    "        self.len_train  = len(self.data_train)\n",
    "        self.len_test   = len(self.data_test)\n",
    "        self.len_train_windows = None\n",
    "    \n",
    "    def get_all_data(self,seq_len,normalise):\n",
    "\n",
    "        data_windows = []\n",
    "        for i in range(self.len_all - seq_len):\n",
    "            data_windows.append(self.all_data[i:i+seq_len])\n",
    "\n",
    "        data_windows = np.array(data_windows).astype(float)\n",
    "\n",
    "        data_windows = self.normalise_windows(data_windows, single_window=False) if normalise else data_windows\n",
    "\n",
    "        x = data_windows[:, :-1,1:]\n",
    "        y = data_windows[:, -1, [0]]\n",
    "        return x,y        \n",
    "\n",
    "\n",
    "    def get_test_data(self, seq_len, normalise,predict_len):\n",
    "\n",
    "        data_x_raise = []\n",
    "        data_y_raise = []\n",
    "        data_x_down = []\n",
    "        data_y_down = []\n",
    "\n",
    "        for i in range(self.len_test - seq_len):   \n",
    "            x, y = self._next_window_test(i, seq_len, normalise)\n",
    "            x0,y0 = self.all_next_window_test(i, seq_len, normalise)\n",
    "            if y0[-(1+predict_len)] >= y0[-(2+predict_len)]:\n",
    "                data_x_raise.append(x)\n",
    "                data_y_raise.append(y)\n",
    "            if y0[-(1+predict_len)] < y0[-(2+predict_len)]:\n",
    "                data_x_down.append(x)\n",
    "                data_y_down.append(y)                \n",
    "        return np.array(data_x_raise), np.array(data_y_raise),np.array(data_x_down), np.array(data_y_down)\n",
    "    \n",
    "    def all_next_window_test(self, i, seq_len, normalise):\n",
    "        window = self.data_test[i:i+seq_len]\n",
    "        window = self.normalise_windows(window, single_window=True)[0] if normalise else window\n",
    "        x = window[:,1:]\n",
    "        y = window[:, [0]]\n",
    "        return x, y    \n",
    "\n",
    "    def _next_window_test(self, i, seq_len, normalise):\n",
    "        window = self.data_test[i:i+seq_len]\n",
    "        window = self.normalise_windows(window, single_window=True)[0] if normalise else window\n",
    "        x = window[:-1,1:]\n",
    "        y = window[-1, [0]]\n",
    "        return x, y\n",
    "\n",
    "    def get_train_data(self, seq_len, normalise,predict_len):\n",
    "\n",
    "        data_x_raise = []\n",
    "        data_y_raise = []\n",
    "        data_x_down = []\n",
    "        data_y_down = []\n",
    "\n",
    "        for i in range(self.len_train - seq_len):   \n",
    "            x, y = self._next_window(i, seq_len, normalise)\n",
    "            x0,y0 = self.all_next_window(i, seq_len, normalise)\n",
    "            if y0[-(1+predict_len)] >= y0[-(2+predict_len)]:\n",
    "                data_x_raise.append(x)\n",
    "                data_y_raise.append(y)\n",
    "            if y0[-(1+predict_len)] < y0[-(2+predict_len)]:\n",
    "                data_x_down.append(x)\n",
    "                data_y_down.append(y)                \n",
    "        return np.array(data_x_raise), np.array(data_y_raise),np.array(data_x_down), np.array(data_y_down)\n",
    "\n",
    "\n",
    "    \n",
    "    def all_next_window(self, i, seq_len, normalise):\n",
    "        window = self.data_train[i:i+seq_len]\n",
    "        window = self.normalise_windows(window, single_window=True)[0] if normalise else window\n",
    "        x = window[:,1:]\n",
    "        y = window[:, [0]]\n",
    "        return x, y    \n",
    "\n",
    "    def _next_window(self, i, seq_len, normalise):\n",
    "        window = self.data_train[i:i+seq_len]\n",
    "        window = self.normalise_windows(window, single_window=True)[0] if normalise else window\n",
    "        x = window[:-1,1:]\n",
    "        y = window[-1, [0]]\n",
    "        return x, y\n",
    "\n",
    "    def normalise_windows(self, window_data, single_window=False):\n",
    "        normalised_data = []\n",
    "        window_data = [window_data] if single_window else window_data\n",
    "\n",
    "        for window in window_data:\n",
    "            normalised_window = []\n",
    "            for col_i in range(window.shape[1]):\n",
    "                normalised_col = [((float(p) / float(window[0, col_i])) - 1) for p in window[:, col_i]]   \n",
    "                normalised_window.append(normalised_col)\n",
    "            normalised_window = np.array(normalised_window).T \n",
    "            normalised_data.append(normalised_window)\n",
    "        return np.array(normalised_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99308dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(predicted_data, true_data):     \n",
    "    fig = plt.figure(facecolor='white')\n",
    "    fig = plt.figure(figsize=(16, 5))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    ax.plot(true_data, label='True Data')\n",
    "    plt.plot(predicted_data, label='Prediction')\n",
    "\n",
    "    ax.set_xlabel('Time Series') \n",
    "    ax.set_ylabel('Water Level')  \n",
    "    plt.legend()\n",
    "    plt.savefig('results_2.png')\n",
    "\n",
    "def GetRMSE(y_hat,y_test):\n",
    "    sum = np.sqrt(metrics.mean_squared_error(y_test, y_hat))\n",
    "    return  sum\n",
    "\n",
    "def GetMAE(y_hat,y_test):\n",
    "    sum = metrics.mean_absolute_error(y_test, y_hat)\n",
    "    return  sum\n",
    "\n",
    "def GetMAPE(y_hat,y_test):\n",
    "    sum = np.mean(np.abs((y_hat - y_test) / y_test)) * 100\n",
    "    return sum\n",
    "\n",
    "def GetMAPE_Order(y_hat,y_test):\n",
    "    zero_index = np.where(y_test == 0)\n",
    "    y_hat = np.delete(y_hat,zero_index[0])\n",
    "    y_test = np.delete(y_test,zero_index[0])\n",
    "    sum = np.mean(np.abs((y_hat - y_test) / y_test)) * 100\n",
    "    return sum\n",
    "\n",
    "def nash_sutcliffe(obs, sim):\n",
    "    obs_mean = np.mean(obs)\n",
    "    numerator = np.sum((obs - sim) ** 2)\n",
    "    denominator = np.sum((obs - obs_mean) ** 2)\n",
    "    nse = 1 - numerator / denominator\n",
    "    return nse\n",
    "\n",
    "def calculate_kge(observations, simulations):\n",
    "    rho = np.corrcoef(observations, simulations)[0, 1]\n",
    "    beta = np.std(simulations) / np.std(observations)\n",
    "    alpha = np.mean(simulations) / np.mean(observations)\n",
    "    kge = 1 - np.sqrt((rho - 1)**2 + (beta - 1)**2 + (alpha - 1)**2)\n",
    "    return kge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84aed50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalize_parameters(train_year, test_year,all_year,filename = \"C:/Users/ljm/Desktop/LSTM模型的各种尝试/data/数据处理/2013-2021,1days训练测试.xlsx\" ):     \n",
    "    data_train = pd.read_excel(filename)\n",
    "    df = data_train.iloc[int((all_year-train_year-test_year) * data_train.shape[0]/all_year) : , 1:]\n",
    "    miu = []\n",
    "    sigma = []\n",
    "    split_propotion = train_year/(train_year+test_year)\n",
    "    for i in range(df.shape[1]):\n",
    "        mean = np.mean(df.iloc[:int(split_propotion*df.shape[0]),i])\n",
    "        variance = np.var(df.iloc[:int(split_propotion*df.shape[0]),i])\n",
    "        miu.append(mean)\n",
    "        sigma.append(variance)\n",
    "    return miu[0],sigma[0]**0.5,split_propotion\n",
    "\n",
    "def F_normalise(prediction,y_test,sigma,miu):\n",
    "    true_predictions_pointbypoint = []\n",
    "    true_y_test = []\n",
    "    for i in range(len(prediction)):\n",
    "        true_predictions_pointbypoint.append(sigma * prediction[i] + miu)        \n",
    "    for j in range(len(prediction)):\n",
    "        true_y_test.append(sigma * y_test[j] + miu)\n",
    "    return true_predictions_pointbypoint,true_y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f25e31",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f688b489",
   "metadata": {},
   "outputs": [],
   "source": [
    "miu,sigma,split_propotion = get_normalize_parameters(train_year = 5,test_year = 3,all_year = 8,filename = \"C:/Users/ljm/Desktop/LSTM模型的各种尝试/data/数据处理/5-3-2-2d-selfLuoshan-train.xlsx\")\n",
    "print(miu,sigma,split_propotion)\n",
    "predict_lenth = 3\n",
    "s_lenghth = [10,30,60]\n",
    "b_size = [4,8,32,128]\n",
    "net1 = [8,32,128]\n",
    "net2 = [8,32,128]\n",
    "net3 = [8,32,128]\n",
    "d_out = [0.2,0.3]\n",
    "grid_params = list(itertools.product(s_lenghth, b_size, net1,net2,net3,d_out))\n",
    "\n",
    "best_MAE = 1000\n",
    "RMSE_with = 1000\n",
    "MAE_list = []\n",
    "for params in grid_params:\n",
    "\tconfigs = {\n",
    "    \t\"data\": {\n",
    "\t\t\t\"filename\": \"Robust0.4-5-3-2-Hankou-train.csv\",\n",
    "\t\t\t\"columns\": [\n",
    "\t\t\t\t\"螺山日均水位\",\n",
    "\t\t\t\t'自相关螺山水位',\n",
    "\t\t\t\t\"枝城日均流量\",\n",
    "\t\t\t\t\"津市（二）日均流量\",\n",
    "\t\t\t\t\"湘潭日均流量\",\n",
    "\t\t\t\t\"桃源日均流量\",\n",
    "\t\t\t\t\"桃江（二）日均流量\"],\n",
    "\t\t\t\"sequence_length\": params[0],\"train_test_split\": split_propotion,\"normalise\": False},\n",
    "\t\t\"training\": {\"epochs\": 1000,\"batch_size\": params[1]},\n",
    "\t\t\"model\": {\"loss\": \"mse\",\"optimizer\": \"adam\",\"save_dir\": \"5-3-2-2-0.4-Hankou-divided\",\n",
    "\t\t\t\"layers\": [\n",
    "\t\t\t\t{\"type\": \"lstm\",\"neurons\": params[2],\"input_timesteps\": params[0]-1,\"input_dim\": 6,\"return_seq\": True},\n",
    "\t\t\t\t{\"type\": \"dropout\",\"rate\": params[5]},\n",
    "\t\t\t\t{\"type\": \"lstm\",\"neurons\": params[3],\"return_seq\": True},\n",
    "\t\t\t\t{\"type\": \"lstm\",\"neurons\": params[4],\"return_seq\": False},\n",
    "\t\t\t\t{\"type\": \"dropout\",\"rate\": params[5]},\n",
    "\t\t\t\t{\"type\": \"dense\",\"neurons\": 1,\"activation\": \"linear\"}]}}\n",
    "\t\n",
    "\tdata = DataLoader(\n",
    "\t\tos.path.join('data', configs['data']['filename']),\n",
    "\t\tconfigs['data']['train_test_split'],\n",
    "\t\tconfigs['data']['columns'])\n",
    "\tmodel = Model()\n",
    "\tmymodel = model.build_model(configs)\n",
    "\tx, y,x2,y2 = data.get_train_data(seq_len=configs['data']['sequence_length'],normalise=configs['data']['normalise'],predict_len = predict_lenth)\n",
    "\tx_test, y_test,x2_test,y2_test = data.get_test_data(seq_len=configs['data']['sequence_length'],normalise=configs['data']['normalise'],predict_len = predict_lenth)\n",
    "\n",
    "\tmodel.train(x,y,x_test,y_test,\n",
    "\tepochs = configs['training']['epochs'],\n",
    "\tbatch_size = configs['training']['batch_size'],\n",
    "\tsave_dir = configs['model']['save_dir'])\n",
    "\t\t\n",
    "\tpredictions_pointbypoint = model.predict_point_by_point(x_test,debug=False)\n",
    "\ttrue_predictions_pointbypoint,true_y_test = F_normalise(prediction = predictions_pointbypoint,y_test=y_test,sigma=sigma,miu=miu)\n",
    "\n",
    "\tmodel = Model()\n",
    "\tmymodel = model.build_model(configs)\n",
    "\tmodel.train(x2,y2,x2_test,y2_test,\n",
    "\tepochs = configs['training']['epochs'],\n",
    "\tbatch_size = configs['training']['batch_size'],\n",
    "\tsave_dir = configs['model']['save_dir'])\n",
    "\n",
    "\tpredictions_pointbypoint = model.predict_point_by_point(x2_test,debug=False)\n",
    "\ttrue_predictions_pointbypoint2,true_y_test2 = F_normalise(prediction = predictions_pointbypoint,y_test=y2_test,sigma=sigma,miu=miu)\n",
    "\tpre = true_predictions_pointbypoint + true_predictions_pointbypoint2\n",
    "\ttrue = true_y_test + true_y_test2\n",
    "\tMAE_index = GetMAE(pre , true)\n",
    "\tMAE_list.append(MAE_index)\n",
    "\tif MAE_index < best_MAE:\n",
    "\t\tbest_MAE = MAE_index\n",
    "\t\tRMSE_with = GetRMSE(true_predictions_pointbypoint, true_y_test)\n",
    "\t\tbest_params = params\n",
    "print(best_MAE)\n",
    "print(RMSE_with)\n",
    "print(best_params)\n",
    "df_MAE = pd.DataFrame(MAE_list)\n",
    "df_MAE.to_excel('MAE_5-3-2-Hankou-divided-0.4.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59327e8e",
   "metadata": {},
   "source": [
    "## Test and Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87beb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_lenth = 3\n",
    "root_dir = \"C:/Users/ljm/Desktop/LSTM模型的各种尝试/selected_model/新的10年实验/5-3-2-2-OnlySelf-divided\"\n",
    "\n",
    "#Different sequence length for different model dicide different figure \n",
    "sequence_length_list = [10, 10]\n",
    "MAE = []\n",
    "RMSE = []\n",
    "NSE = []\n",
    "num = 0\n",
    "sum_prediction = []\n",
    "for root, dirs, files in os.walk(root_dir):\n",
    "    for file in files:\n",
    "        file_name = file\n",
    "        model_one = load_model(os.path.join(root_dir, file_name))\n",
    "        data = DataLoader(os.path.join('data', configs['data']['filename']),configs['data']['train_test_split'],\n",
    "        configs['data']['columns'])\n",
    "\n",
    "        x_test, y_test,x2_test,y2_test,order_list,lenth = data.get_all_data(seq_len=sequence_length_list[num],normalise=configs['data']['normalise'],predict_len = predict_lenth)\n",
    "\n",
    "        if num%2 == 0:\n",
    "            predictions_pointbypoint = model_one.predict(x_test)\n",
    "            true_predictions_pointbypoint,true_y_test = F_normalise(prediction = predictions_pointbypoint,y_test=y_test,sigma=sigma,miu=miu)\n",
    "            prediction = true_predictions_pointbypoint\n",
    "            reality = true_y_test\n",
    "        if num%2 == 1:\n",
    "            predictions_pointbypoint = model_one.predict(x2_test)\n",
    "            true_predictions_pointbypoint,true_y_test = F_normalise(prediction = predictions_pointbypoint,y_test=y2_test,sigma=sigma,miu=miu)\n",
    "            prediction += true_predictions_pointbypoint\n",
    "            reality += true_y_test\n",
    "        \n",
    "            MAE.append(GetMAE(prediction, reality))\n",
    "            RMSE.append(GetRMSE(prediction, reality))\n",
    "            NSE.append(nash_sutcliffe(reality,prediction))\n",
    "\n",
    "            high_level_true = []\n",
    "            high_level_pre = []\n",
    "            ordinary_level_true = []\n",
    "            ordinary_level_pre = []\n",
    "            low_level_true = []\n",
    "            low_level_pre = []\n",
    "            for i  in range (len(reality)):\n",
    "                if reality[i] >= (miu + sigma):\n",
    "                    high_level_true.append(reality[i])\n",
    "                    high_level_pre.append(prediction[i])\n",
    "                \n",
    "                if reality[i] <= (miu - sigma):\n",
    "                    low_level_true.append(reality[i])\n",
    "                    low_level_pre.append(prediction[i])\n",
    "                \n",
    "                else:\n",
    "                    ordinary_level_true.append(reality[i])\n",
    "                    ordinary_level_pre.append(prediction[i])\n",
    "\n",
    "            print('High',GetMAE(high_level_pre, high_level_true), GetRMSE(high_level_pre, high_level_true))\n",
    "            print('Ordinary',GetMAE(ordinary_level_pre, ordinary_level_true), GetRMSE(ordinary_level_pre, ordinary_level_true))\n",
    "            print('Low',GetMAE(low_level_pre, low_level_true), GetRMSE(low_level_pre, low_level_true))\n",
    "\n",
    "\n",
    "            #Ensemble Model\n",
    "            order_list_predict = [0] * lenth\n",
    "            order_list_true = [0] *  lenth\n",
    "            for i in range (len(order_list)): \n",
    "                order = order_list[i]\n",
    "                order_list_predict[order] = prediction[i]\n",
    "                order_list_true[order] = reality[i]\n",
    "\n",
    "            largest_sequence = max(sequence_length_list)\n",
    "            order_list_predict = order_list_predict[(largest_sequence-sequence_length_list[num]):]\n",
    "\n",
    "            if num == 1:\n",
    "                sum_prediction = order_list_predict\n",
    "            else :\n",
    "                list = []\n",
    "                for i in range(len(sum_prediction)):\n",
    "                    list.append((sum_prediction[i]+order_list_predict[i]))\n",
    "                sum_prediction = list\n",
    "            order_list_true = order_list_true[(largest_sequence-sequence_length_list[num]):]\n",
    "\n",
    "\n",
    "        num+=1\n",
    "    \n",
    "average_prediction = []\n",
    "for i in range(len(sum_prediction)):\n",
    "    average_prediction.append((sum_prediction[i]/(num/2)))\n",
    "plot_results(average_prediction,order_list_true)\n",
    "print(\"全部数据MAE(绝对误差的平均值）为\", GetMAE(average_prediction, order_list_true))\n",
    "print(\"全部数据RMSE（均方根误差）为\", GetRMSE(average_prediction, order_list_true))\n",
    "print(\"NSE为\",nash_sutcliffe(order_list_true,average_prediction))\n",
    "\n",
    "MAE_df = pd.DataFrame(MAE)\n",
    "RMSE_df = pd.DataFrame(RMSE)\n",
    "NSE_df = pd.DataFrame(NSE)\n",
    "MAE_df = MAE_df.rename(columns={0:'MAE'})\n",
    "RMSE_df = RMSE_df.rename(columns={0:'RMSE'})\n",
    "result = MAE_df.join(RMSE_df).join(NSE_df) \n",
    "result.to_excel('Result.xlsx',index  = False)                     \n",
    "print(MAE)\n",
    "print(RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ececef4",
   "metadata": {},
   "source": [
    "### Plot Figure6 in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfd6c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#查看涨落水各自的精度\n",
    "predict_lenth = 3\n",
    "\n",
    "root_dir = 'C:/Users/ljm/Desktop/LSTM模型的各种尝试/selected_model/新的10年实验/5-3-2-2-self-divided/best'\n",
    "\n",
    "#不同模型需要改这个 \n",
    "sequence_length_list = [10, 10]\n",
    "\n",
    "file_name = '1-1.h5'\n",
    "model_one = load_model(os.path.join(root_dir, file_name))\n",
    "data = DataLoader(os.path.join('data', configs['data']['filename']),configs['data']['train_test_split'],\n",
    "configs['data']['columns'])\n",
    "\n",
    "#预测集用all,训练集用test\n",
    "x_test, y_test,x2_test,y2_test,order_list,lenth = data.get_all_data(seq_len=sequence_length_list[0],normalise=configs['data']['normalise'],predict_len = predict_lenth)\n",
    "\n",
    "predictions_pointbypoint = model_one.predict(x2_test)\n",
    "true_predictions_pointbypoint,true_y_test = F_normalise(prediction = predictions_pointbypoint,y_test=y2_test,sigma=sigma,miu=miu)\n",
    "\n",
    "file_name = '1-2.h5'\n",
    "model_one = load_model(os.path.join(root_dir, file_name))\n",
    "data = DataLoader(os.path.join('data', configs['data']['filename']),configs['data']['train_test_split'],\n",
    "configs['data']['columns'])\n",
    "predictions_pointbypoint = model_one.predict(x2_test)\n",
    "true_predictions_pointbypoint2,true_y_test2 = F_normalise(prediction = predictions_pointbypoint,y_test=y2_test,sigma=sigma,miu=miu)\n",
    "\n",
    "print(GetMAE(true_predictions_pointbypoint, true_y_test))\n",
    "print(GetMAE(true_predictions_pointbypoint2, true_y_test2))\n",
    "\n",
    "\n",
    "X = [i for i in range(len(true_predictions_pointbypoint))]\n",
    "import matplotlib.lines as mlines\n",
    "#散点图\n",
    "fig = plt.figure(figsize=(11, 8))\n",
    "plt.scatter(X, true_predictions_pointbypoint, s=5, c='orange',label='Rising' ,marker='^', vmax=None, alpha=None, linewidths=None,  edgecolors=None, plotnonfinite=False, data=None)\n",
    "plt.scatter(X, true_predictions_pointbypoint2, s=5, c='b', label='Recession' ,marker='s', vmax=None, alpha=None, linewidths=None,  edgecolors=None, plotnonfinite=False, data=None)\n",
    "plt.ylabel('Water Level(m)', fontsize=16, fontweight='bold')  # y 轴标签\n",
    "\n",
    "\n",
    "legend_handle1 = mlines.Line2D([], [], color='orange', marker='^', linestyle='None',\n",
    "                              markersize=7, label='Rising')  # 调整markersize以改变图例中的标记大小\n",
    "legend_handle2 = mlines.Line2D([], [], color='blue', marker='s', linestyle='None',\n",
    "                              markersize=7, label='Recession')  # 调整markersize以改变图例中的标记大小\n",
    "\n",
    "plt.legend(handles=[legend_handle1, legend_handle2], loc='upper right', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1484f2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
